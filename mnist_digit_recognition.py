# -*- coding: utf-8 -*-
"""CNN_hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IfoisO0-fG2guwvcB6otDshi-Nyq3bVm

## UT Dallas CS 6301.502.18F 
Special Topics in Computer Science

Convolutional Neural Networks

Homework 2

MNIST Digit Recognition

Patrick Dayton

Due 17 September 2018
"""

# import necessary packages
!pip install torch
!pip install torchvision
import os
import torch
import torch.nn as nn
from torch.autograd import Variable
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

# set variables
numClasses = 10
learning_rate = 0.1
n_epochs = 10
root = './data'
batch_size_test = 1000
batch_size_train = 64
momentum = .5
seed = 1
log_interval = 100

# set our data transform
trans = transforms.Compose(
    [transforms.ToTensor(), 
     transforms.Normalize((0.5,), (1.0,))])

# Load PyTorch's built in MNIST data set and apply the above tranform.
train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)
test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)

# Set up dataloaders which help batch and shuffle data.
train_loader = torch.utils.data.DataLoader( train_set,
  batch_size=batch_size_train, shuffle=True)
print(type(train_loader))

test_loader = torch.utils.data.DataLoader( test_set,
  batch_size=batch_size_test, shuffle=True)

# Sanity check of dataset sizes and shape
examples = enumerate(test_loader)
batch_idx, (example_data, example_targets) = next(examples)
print(len(test_loader.dataset)) # 10000
print(len(train_loader.dataset)) # 60000
print(next(examples)[1][0].shape) # check shape of data images

print('One batch of data has the shape: ', example_data.shape)

# Display a few MNIST images and their true values with matplotlib
fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("True Value: {}".format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])
fig

# Original Net used in the tutorial
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # Convolutional Layer
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50) # Fully Connected Layer
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)

# Defining Net from Homework Parameters
class HomeworkNet(nn.Module):
    def __init__(self):
        super(HomeworkNet, self).__init__()
        self.fc1 = nn.Linear(784, 512)
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = x.view(-1, 28*28)  # resizing to be a 1x784 tensor
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)

network = HomeworkNet()
optimizer = optim.SGD(network.parameters(), 
                      lr=learning_rate,
                      momentum=momentum)

train_losses = []
train_counter = []
test_losses = []
test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]

# Train the network for a given number of epochs
def train(epoch):
  network.train()
  for batch_idx, (data, target) in enumerate(train_loader):
    optimizer.zero_grad()
    output = network(data)
    loss = F.nll_loss(output, target)
    loss.backward()
    optimizer.step()
    if batch_idx % log_interval == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
        epoch, batch_idx * len(data), len(train_loader.dataset),
        100. * batch_idx / len(train_loader), loss.item()))
      train_losses.append(loss.item())
      train_counter.append(
        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))

# test the network
def test():
  network.eval()
  test_loss = 0
  correct = 0
  with torch.no_grad():
    for data, target in test_loader:
      output = network(data)
      test_loss += F.nll_loss(output, target, size_average=False).item()
      pred = output.data.max(1, keepdim=True)[1]
      correct += pred.eq(target.data.view_as(pred)).sum()
  test_loss /= len(test_loader.dataset)
  test_losses.append(test_loss)
  print('In our test set, we should expect around 10% accuracy.')
  print('\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
    test_loss, correct, len(test_loader.dataset),
    100. * correct / len(test_loader.dataset)))

# Get a baseline by running the network on our test data without training
test()

# Train the network and run the test again for the number of epochs specified
for epoch in range(1, n_epochs + 1):
  train(epoch)
  test()

with torch.no_grad():
  output = network(example_data)

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Prediction: {}".format(
    output.data.max(1, keepdim=True)[1][i].item()))
  plt.xticks([])
  plt.yticks([])
fig

